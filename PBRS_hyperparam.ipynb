{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4861e986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import protector as protect\n",
    "from utils.cli_utils import softmax_ent\n",
    "\n",
    "from tent import Tent, configure_model, collect_params\n",
    "from typing import Sequence, Tuple, Dict, Optional\n",
    "import argparse\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utilities import *  ## created by me\n",
    "from plotting import *  ## created by me\n",
    "from LabelShiftExperiments import *  ## created by me\n",
    "from protector import *  ## created by me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a45f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRUPTIONS = (\n",
    "    \"shot_noise\",\n",
    "    \"motion_blur\",\n",
    "    \"snow\",\n",
    "    \"pixelate\",\n",
    "    \"gaussian_noise\",\n",
    "    \"defocus_blur\",\n",
    "    \"brightness\",\n",
    "    \"fog\",\n",
    "    \"zoom_blur\",\n",
    "    \"frost\",\n",
    "    \"glass_blur\",\n",
    "    \"impulse_noise\",\n",
    "    \"contrast\",\n",
    "    \"jpeg_compression\",\n",
    "    \"elastic_transform\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506c79e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENTER PARAMETERS ##\n",
    "\n",
    "# Manual settings for arguments\n",
    "args = type(\"Args\", (), {})()  # Create a simple namespace object\n",
    "args.device = \"cpu\"  # Change this manually as needed\n",
    "args.method = \"none\"  # Options: 'none' or 'tent'\n",
    "args.corruption = \"gaussian_noise\"  # Choose from CORRUPTIONS\n",
    "args.all_corruptions = False  # Set to True to test all corruptions\n",
    "args.n_examples = 1000\n",
    "args.batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c7e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically set device to best available option\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Define normalization transform using CIFAR-10 mean and std values\n",
    "transform = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616))\n",
    "\n",
    "# Load pre-trained model and move to appropriate device\n",
    "print(\"ðŸš€ Loading model...\")\n",
    "model = get_model(args.method, device)\n",
    "\n",
    "# Load clean CIFAR-10 test data to compute source entropies\n",
    "print(\"ðŸ“¦ Loading clean CIFAR-10 as source entropy\")\n",
    "clean_ds = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transforms.Compose([transforms.ToTensor(), transform])\n",
    ")\n",
    "clean_loader = DataLoader(clean_ds, batch_size=args.batch_size, shuffle=False)\n",
    "source_ents, accuracy, logits_list, labels_list = evaluate(model, clean_loader, device)\n",
    "\n",
    "# Initialize protector with source entropies for shift detection\n",
    "protector = protect.get_protector_from_ents(\n",
    "    source_ents, argparse.Namespace(gamma=1 / (8 * np.sqrt(3)), eps_clip=1.8, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import optuna\n",
    "\n",
    "def run_optuna_pbrs_optimization(model,\n",
    "                                  compare_fpr_across_seeds,\n",
    "                                  evaluate_covariate_shift_detection,\n",
    "                                  load_cifar10_corruption,\n",
    "                                  load_cifar10_label_shift_balanced,\n",
    "                                  BasicDataset,\n",
    "                                  run_martingale,\n",
    "                                  protector_factory,\n",
    "                                  transform,\n",
    "                                  args,\n",
    "                                  device,\n",
    "                                  corruption_types,\n",
    "                                  severities,\n",
    "                                  num_classes_list=[1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "                                  seeds_tpr=range(3),\n",
    "                                  seeds_fpr=range(3),\n",
    "                                  n_trials=50,\n",
    "                                  log_path='optuna_pbrs_results.csv'):\n",
    "    \n",
    "    run_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results = []\n",
    "\n",
    "    def objective(trial):\n",
    "        buffer_size = trial.suggest_categorical(\"buffer_size\", [32, 64, 128, 256, 512, 1024])\n",
    "        confidence_threshold = trial.suggest_float(\"confidence_threshold\", 0.5, 0.95, step=0.05)\n",
    "        gamma = trial.suggest_categorical(\"gamma\", [1 / (16 * 3 ** 0.5), 1 / (8 * 3 ** 0.5), 1 / (4 * 3 ** 0.5)])\n",
    "        eps_clip = trial.suggest_categorical(\"eps_clip\", [1.5, 1.8, 2.0, 2.5, 3.0])\n",
    "\n",
    "        run_id = f\"{run_time}_B{buffer_size}_T{confidence_threshold:.2f}_G{gamma:.3f}_E{eps_clip:.2f}\"\n",
    "        print(f\"\\nðŸ”§ Trial {trial.number}: {run_id}\")\n",
    "\n",
    "        # === Step 1: Deepcopy the model to ensure reproducibility ===\n",
    "        model_copy_tpr = deepcopy(model)\n",
    "        model_copy_fpr = deepcopy(model)\n",
    "\n",
    "        # === Step 2: Recompute source entropy for this trial ===\n",
    "        clean_loader = DataLoader(\n",
    "            torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True,\n",
    "                                        transform=torchvision.transforms.Compose([\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "                                            transform\n",
    "                                        ])),\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        source_ents, _, _, _ = evaluate(model_copy_tpr, clean_loader, device)\n",
    "\n",
    "        # === Step 3: Create protector using clean entropies ===\n",
    "        args_protector = argparse.Namespace(gamma=gamma, eps_clip=eps_clip, device=device)\n",
    "        protector_tpr = protect.get_protector_from_ents(source_ents, args_protector)\n",
    "        protector_fpr = protect.get_protector_from_ents(source_ents, args_protector)\n",
    "\n",
    "        # === Step 4: Evaluate TPR ===\n",
    "        tpr_result = evaluate_covariate_shift_detection(\n",
    "            model=model_copy_tpr,\n",
    "            load_cifar10_corruption=load_cifar10_corruption,\n",
    "            BasicDataset=BasicDataset,\n",
    "            run_martingale=run_martingale,\n",
    "            protector=protector_tpr,\n",
    "            transform=transform,\n",
    "            args=args,\n",
    "            device=device,\n",
    "            corruption_types=corruption_types,\n",
    "            severities=severities,\n",
    "            seeds=seeds_tpr,\n",
    "            buffer_capacity=buffer_size,\n",
    "            confidence_threshold=confidence_threshold,\n",
    "            num_classes=10,\n",
    "            use_pbrs=True,\n",
    "            log_path=f'tpr_results_{run_id}.csv'\n",
    "        )\n",
    "\n",
    "        tpr_scores = [v['detection_rate'] for v in tpr_result.values()]\n",
    "        delays = [v['avg_delay'] for v in tpr_result.values() if v['avg_delay'] is not None]\n",
    "        mean_tpr = float(pd.Series(tpr_scores).mean())\n",
    "        mean_delay = float(pd.Series(delays).mean()) if delays else 4000\n",
    "\n",
    "        # === Step 5: Evaluate FPR ===\n",
    "        fpr_result = compare_fpr_across_seeds(\n",
    "            model=model_copy_fpr,\n",
    "            load_cifar10_label_shift_balanced=load_cifar10_label_shift_balanced,\n",
    "            BasicDataset=BasicDataset,\n",
    "            run_martingale=run_martingale,\n",
    "            protector=protector_fpr,\n",
    "            transform=transform,\n",
    "            args=args,\n",
    "            device=device,\n",
    "            seeds=seeds_fpr,\n",
    "            num_classes_list=num_classes_list,\n",
    "            buffer_capacity=buffer_size,\n",
    "            confidence_threshold=confidence_threshold,\n",
    "            use_pbrs=True,\n",
    "            log_path=f'fpr_results_{run_id}.csv'\n",
    "        )\n",
    "\n",
    "        fpr_scores = list(fpr_result.values())\n",
    "        mean_fpr = float(pd.Series(fpr_scores).mean())\n",
    "\n",
    "        # === Step 6: Compute objective ===\n",
    "        score = (0.8 * (1 - mean_fpr)) + (0.9 * mean_tpr) - (0.3 * (mean_delay / 4000))\n",
    "        trial.set_user_attr(\"fpr\", mean_fpr)\n",
    "        trial.set_user_attr(\"tpr\", mean_tpr)\n",
    "        trial.set_user_attr(\"delay\", mean_delay)\n",
    "\n",
    "        results.append({\n",
    "            'run_id': run_id,\n",
    "            'trial': trial.number,\n",
    "            'buffer_size': buffer_size,\n",
    "            'confidence_threshold': confidence_threshold,\n",
    "            'gamma': gamma,\n",
    "            'eps_clip': eps_clip,\n",
    "            'mean_fpr': mean_fpr,\n",
    "            'mean_tpr': mean_tpr,\n",
    "            'avg_delay': mean_delay,\n",
    "            'score': score\n",
    "        })\n",
    "\n",
    "        print(f\"ðŸ”Ž Trial {trial.number}: score = {score:.3f} | FPR={mean_fpr:.3f} | TPR={mean_tpr:.3f} | Delay={mean_delay:.0f}\")\n",
    "        return score\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(log_path, index=False)\n",
    "    print(f\"\\nâœ… Optuna results saved to {log_path}\")\n",
    "    print(\"Best trial:\", study.best_trial.params)\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d1dbaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
