{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import protector as protect\n",
    "from utils.cli_utils import softmax_ent\n",
    "\n",
    "from tent import Tent, configure_model, collect_params\n",
    "from typing import Sequence, Tuple, Dict, Optional\n",
    "import argparse\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utilities import *  ## created by me\n",
    "from plotting import *  ## created by me\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRUPTIONS = (\n",
    "    \"shot_noise\",\n",
    "    \"motion_blur\",\n",
    "    \"snow\",\n",
    "    \"pixelate\",\n",
    "    \"gaussian_noise\",\n",
    "    \"defocus_blur\",\n",
    "    \"brightness\",\n",
    "    \"fog\",\n",
    "    \"zoom_blur\",\n",
    "    \"frost\",\n",
    "    \"glass_blur\",\n",
    "    \"impulse_noise\",\n",
    "    \"contrast\",\n",
    "    \"jpeg_compression\",\n",
    "    \"elastic_transform\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENTER PARAMETERS ##\n",
    "\n",
    "# Manual settings for arguments\n",
    "args = type(\"Args\", (), {})()  # Create a simple namespace object\n",
    "args.device = \"cpu\"  # Change this manually as needed\n",
    "args.method = \"none\"  # Options: 'none' or 'tent'\n",
    "args.corruption = \"gaussian_noise\"  # Choose from CORRUPTIONS\n",
    "args.all_corruptions = False  # Set to True to test all corruptions\n",
    "args.n_examples = 1000\n",
    "args.batch_size = 64\n",
    "\n",
    "# Set torch seed for replicability (don't know if this preserves consistency when using different devices)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic set up where we load clean CIFAR-10 and then test on corrupted version. This is a good reference to get a feel for how everyting works together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Mikel/.cache\\torch\\hub\\chenyaofo_pytorch-cifar-models_master\n"
     ]
    }
   ],
   "source": [
    "# Dynamically set device to best available option\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Define normalization transform using CIFAR-10 mean and std values\n",
    "transform = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616))\n",
    "\n",
    "# Load pre-trained model and move to appropriate device\n",
    "print(\"üöÄ Loading model...\")\n",
    "model = get_model(args.method, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BBSE ODS False Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Evaluating label shift (2-class): (0, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-22 14:15:24.900\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprotector\u001b[0m:\u001b[36mset_gamma\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1msetting gamma val to 0.07216878364870323\u001b[0m\n",
      "\u001b[32m2025-05-22 14:15:24.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprotector\u001b[0m:\u001b[36mset_eps_clip_val\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1msetting epsilon clip val to 1.8\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset: (0, 1)\n",
      "p_source: [0.49807933 0.5019206  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "p_test_true: [0.5026567  0.49734333 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "min p_source: 0.0 min p_t: 0.0\n",
      "\n",
      "üîé Evaluating label shift (2-class): (0, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-22 14:16:23.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprotector\u001b[0m:\u001b[36mset_gamma\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1msetting gamma val to 0.07216878364870323\u001b[0m\n",
      "\u001b[32m2025-05-22 14:16:23.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprotector\u001b[0m:\u001b[36mset_eps_clip_val\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1msetting epsilon clip val to 1.8\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset: (0, 2)\n",
      "p_source: [0.47730058 0.         0.52269936 0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "p_test_true: [0.5079023  0.         0.49209768 0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "min p_source: 0.0 min p_t: 0.0\n",
      "\n",
      "üîé Evaluating label shift (2-class): (0, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-22 14:17:22.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprotector\u001b[0m:\u001b[36mset_gamma\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1msetting gamma val to 0.07216878364870323\u001b[0m\n",
      "\u001b[32m2025-05-22 14:17:22.382\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprotector\u001b[0m:\u001b[36mset_eps_clip_val\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1msetting epsilon clip val to 1.8\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset: (0, 3)\n",
      "p_source: [0.49999997 0.         0.         0.49999997 0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "p_test_true: [0.5061103  0.         0.         0.49388963 0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "min p_source: 0.0 min p_t: 0.0\n",
      "\n",
      "‚û°Ô∏è 0 / 3 (2-class) shifts triggered detection ‚Äî FPR = 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from weighted_cdf_bbse_ods import (\n",
    "    BBSEODSWeightedCDF,\n",
    "    estimate_confusion_matrix,\n",
    "    estimate_target_distribution_from_preds,\n",
    ")\n",
    "\n",
    "fpr_by_size = {}\n",
    "\n",
    "for num_classes in [2]:  # Adjust as needed\n",
    "    candidate_class_sets = list(combinations(range(10), num_classes))\n",
    "    entropy_peaks = {}\n",
    "    threshold_crossed = {}\n",
    "\n",
    "    for subset in candidate_class_sets[:3]:\n",
    "        print(f\"\\nüîé Evaluating label shift ({num_classes}-class): {subset}\")\n",
    "\n",
    "        split = 4000\n",
    "\n",
    "        # 1) Load clean source stream using only this subset of classes\n",
    "        x_src, y_src = load_cifar10_label_shift(keep_classes=subset, n_examples=8000, shift_point=split)\n",
    "        source_dataset = BasicDataset(x_src, y_src, transform=transform)\n",
    "\n",
    "        source_loader = DataLoader(Subset(source_dataset, range(0, split)), batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "        source_ents = []\n",
    "        source_labels = []\n",
    "        with torch.no_grad():\n",
    "            for x_batch, labels in source_loader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                logits = model(x_batch)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)\n",
    "                y_hat = torch.argmax(probs, dim=1)\n",
    "\n",
    "                source_ents.extend(entropy.cpu().tolist())\n",
    "                # We have access to ground truth labels for init of CDF, so we are using them here\n",
    "                source_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "        # BBSE: Estimate confusion matrix and p_source from clean source stream\n",
    "        confusion_matrix, p_source = estimate_confusion_matrix(model, source_loader, device)\n",
    "\n",
    "        # Load label-shifted stream (same subset)\n",
    "        x_shift, y_shift = load_cifar10_label_shift(keep_classes=subset, n_examples=8000, shift_point=split)\n",
    "        full_dataset = BasicDataset(x_shift, y_shift, transform=transform)\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            Subset(full_dataset, range(split, len(full_dataset))), batch_size=args.batch_size, shuffle=False\n",
    "        )\n",
    "\n",
    "        # 4. Estimate p_test from shifted half of the stream\n",
    "        p_test = estimate_target_distribution_from_preds(model, test_loader, device)\n",
    "\n",
    "        # 5. Create BBSE/ODS weighted protector\n",
    "        protector = protect.get_bbse_ods_weighted_protector_from_ents(\n",
    "            source_ents,\n",
    "            p_test,\n",
    "            p_source,\n",
    "            source_labels,\n",
    "            confusion_matrix,\n",
    "            0.05,  # ods_alpha\n",
    "            argparse.Namespace(gamma=1 / (8 * np.sqrt(3)), eps_clip=1.8, device=device),\n",
    "        )\n",
    "\n",
    "        # LOUIS: This is for stat tracking i guess?\n",
    "        mask = torch.tensor([i in subset for i in range(10)], dtype=torch.bool)\n",
    "        p_source = p_source.to(\"cpu\") * mask\n",
    "        p_source /= p_source.sum() + 1e-8\n",
    "\n",
    "        p_test_true = protector.cdf.p_test_true.to(\"cpu\") * mask\n",
    "        p_test_true /= p_test_true.sum() + 1e-8\n",
    "\n",
    "        # 6) Evaluate full test stream and run martingale\n",
    "        loader = DataLoader(full_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "        # ents, acc, logits_list, labels_list = evaluate(model, loader, device)\n",
    "\n",
    "        # Initialize tracking variables\n",
    "        entropies = []\n",
    "        correct, total = 0, 0\n",
    "        logits_list, labels_list = [], []\n",
    "        logs, eps = [], []\n",
    "\n",
    "        # Reset the protector for a fresh start\n",
    "        protector.reset()\n",
    "\n",
    "        # Process batch-by-batch and update ODS as we go\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                logits = model(x)\n",
    "                batch_entropies = softmax_ent(logits).tolist()\n",
    "\n",
    "                # Get predictions and update accuracy tracking\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                correct += (preds == y).sum().item()\n",
    "                total += y.size(0)\n",
    "\n",
    "                # Store logits and labels for later analysis\n",
    "                logits_list.append(logits.cpu())\n",
    "                labels_list.append(y.cpu())\n",
    "\n",
    "                # KEY STEP: Update protector with new pseudo-labels (ODS update)\n",
    "                protector.cdf.batch_ods_update(preds.cpu().tolist())\n",
    "\n",
    "                # Process each entropy value through the martingale detector\n",
    "                for z in batch_entropies:\n",
    "                    # Get CDF value (which now uses updated weights)\n",
    "                    u = protector.cdf(z)\n",
    "                    # Update martingale\n",
    "                    protector.protect_u(u)\n",
    "                    # Store results\n",
    "                    logs.append(np.log(protector.martingales[-1] + 1e-8))\n",
    "                    eps.append(protector.epsilons[-1])\n",
    "                    # Store entropy for later reference\n",
    "                    entropies.append(z)\n",
    "\n",
    "        # Finalize results\n",
    "        acc = correct / total\n",
    "        key = f\"labelshift_{num_classes}cls_{'_'.join(map(str, subset))}\"\n",
    "        ents = np.array(entropies)\n",
    "        results = {key: {\"log_sj\": logs, \"eps\": eps}}\n",
    "        result = results[key]\n",
    "\n",
    "        # Record statistics for the main analysis\n",
    "        entropy_peaks[key] = np.max(ents)\n",
    "        threshold_crossed[key] = np.max(result[\"log_sj\"]) > np.log(100)\n",
    "\n",
    "        print(\"Subset:\", subset)\n",
    "        print(\"p_source:\", p_source.numpy())\n",
    "        print(\"p_test_true:\", p_test_true.numpy())\n",
    "        print(\"min p_source:\", p_source.min().item(), \"min p_t:\", p_test_true.min().item())\n",
    "\n",
    "    # 8) Calculate and store FPR\n",
    "    n_tests = len(threshold_crossed)\n",
    "    n_crossed = sum(threshold_crossed.values())\n",
    "    fpr = n_crossed / n_tests\n",
    "    fpr_by_size[num_classes] = fpr\n",
    "\n",
    "    print(f\"\\n‚û°Ô∏è {n_crossed} / {n_tests} ({num_classes}-class) shifts triggered detection ‚Äî FPR = {fpr:.3f}\\n\")\n",
    "\n",
    "# fpr_by_size now holds clean, valid FPRs per class subset size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cifar100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
