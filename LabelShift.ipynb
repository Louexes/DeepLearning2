{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import protector as protect\n",
    "from utils.cli_utils import softmax_ent\n",
    "\n",
    "from tent import Tent, configure_model, collect_params\n",
    "from typing import Sequence, Tuple, Dict, Optional\n",
    "import argparse\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utilities import *  ## created by me\n",
    "from plotting import *  ## created by me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRUPTIONS = (\n",
    "    \"shot_noise\",\n",
    "    \"motion_blur\",\n",
    "    \"snow\",\n",
    "    \"pixelate\",\n",
    "    \"gaussian_noise\",\n",
    "    \"defocus_blur\",\n",
    "    \"brightness\",\n",
    "    \"fog\",\n",
    "    \"zoom_blur\",\n",
    "    \"frost\",\n",
    "    \"glass_blur\",\n",
    "    \"impulse_noise\",\n",
    "    \"contrast\",\n",
    "    \"jpeg_compression\",\n",
    "    \"elastic_transform\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENTER PARAMETERS ##\n",
    "\n",
    "# Manual settings for arguments\n",
    "args = type(\"Args\", (), {})()  # Create a simple namespace object\n",
    "args.device = \"cpu\"  # Change this manually as needed\n",
    "args.method = \"none\"  # Options: 'none' or 'tent'\n",
    "args.corruption = \"gaussian_noise\"  # Choose from CORRUPTIONS\n",
    "args.all_corruptions = False  # Set to True to test all corruptions\n",
    "args.n_examples = 1000\n",
    "args.batch_size = 64\n",
    "\n",
    "# Set torch seed for replicability (don't know if this preserves consistency when using different devices)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic set up where we load clean CIFAR-10 and then test on corrupted version. This is a good reference to get a feel for how everyting works together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically set device to best available option\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Define normalization transform using CIFAR-10 mean and std values\n",
    "transform = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616))\n",
    "\n",
    "# Load pre-trained model and move to appropriate device\n",
    "print(\"üöÄ Loading model...\")\n",
    "model = get_model(args.method, device)\n",
    "\n",
    "# Load clean CIFAR-10 test data to compute source entropies\n",
    "print(\"üì¶ Loading clean CIFAR-10 as source entropy\")\n",
    "clean_ds = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transforms.Compose([transforms.ToTensor(), transform])\n",
    ")\n",
    "clean_loader = DataLoader(clean_ds, batch_size=args.batch_size, shuffle=False)\n",
    "source_ents, _, _, _ = evaluate(model, clean_loader, device)\n",
    "\n",
    "# Initialize protector with source entropies for shift detection\n",
    "protector = protect.get_protector_from_ents(\n",
    "    source_ents, argparse.Namespace(gamma=1 / (8 * np.sqrt(3)), eps_clip=1.8, device=device)\n",
    ")\n",
    "\n",
    "# --- Label Shift Severity Sweep ---\n",
    "label_shift_severities = {\n",
    "    \"none\": list(range(10)),  # no actual shift\n",
    "    \"medium\": list(range(5)),  # moderate shift\n",
    "    \"severe\": [0, 1, 2],  # strong shift\n",
    "    \"extreme\": [0],  # extreme shift\n",
    "}\n",
    "\n",
    "entropy_streams = {}\n",
    "accs = {}\n",
    "logits_list_dict = {}\n",
    "logits_labels_dict = {}\n",
    "\n",
    "for severity_name, keep_classes in label_shift_severities.items():\n",
    "    print(f\"üîé Evaluating label shift severity: {severity_name}\")\n",
    "    x, y = load_cifar10_label_shift(keep_classes=keep_classes, n_examples=8000, shift_point=4000)\n",
    "    dataset = BasicDataset(x, y, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    ents, acc, logits_list, labels_list = evaluate(model, loader, device)\n",
    "\n",
    "    key = f\"label_shift_{severity_name}\"\n",
    "    entropy_streams[key] = ents\n",
    "    accs[key] = acc\n",
    "    logits_list_dict[key] = logits_list\n",
    "    logits_labels_dict[key] = labels_list\n",
    "\n",
    "# Run martingale-based shift detection\n",
    "results = run_martingale(entropy_streams, protector)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nüìä Accuracy under label shift severities:\")\n",
    "for key, value in accs.items():\n",
    "    print(f\"{key}: {value * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot entropy streams for each severity level\n",
    "plt.plot(entropy_streams[\"label_shift_none\"], label=\"None\", alpha=0.7)\n",
    "plt.plot(entropy_streams[\"label_shift_medium\"], label=\"Medium\", alpha=0.7)\n",
    "plt.plot(entropy_streams[\"label_shift_severe\"], label=\"Severe\", alpha=0.7)\n",
    "\n",
    "# Add vertical line at shift point (assuming shift point at 4000/batch_size)\n",
    "shift_point = 4000\n",
    "plt.axvline(x=shift_point, color=\"r\", linestyle=\"--\", alpha=0.5, label=\"Shift Point\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel(\"Batch Index\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.title(\"Entropy Streams for Different Label Shift Severities\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_shift_levels = {\n",
    "    \"mild\": list(range(10)),\n",
    "    \"medium\": list(range(5)),\n",
    "    \"severe\": [0, 1, 2],\n",
    "    \"test\": [6, 7, 8],\n",
    "    \"test2\": [3, 4, 5],\n",
    "    \"extreme\": [0],\n",
    "}\n",
    "\n",
    "accs, entropy_streams, logits_list_dict, logits_labels_dict = {}, {}, {}, {}\n",
    "\n",
    "for severity_name, keep_classes in label_shift_levels.items():\n",
    "    print(f\"üîé Label shift severity: {severity_name}\")\n",
    "\n",
    "    loader, is_clean, labels = load_clean_then_label_shift_sequence(\n",
    "        keep_classes=keep_classes,\n",
    "        n_examples=8000,\n",
    "        shift_point=4000,\n",
    "        data_dir=\"./data/cifar-10-batches-py\",\n",
    "        transform=transform,\n",
    "        batch_size=args.batch_size,\n",
    "    )\n",
    "\n",
    "    ents, acc, logits_list, labels_list = evaluate(model, loader, device)\n",
    "\n",
    "    key = f\"labelshift_{severity_name}\"\n",
    "    entropy_streams[key] = ents\n",
    "    accs[key] = acc\n",
    "    logits_list_dict[key] = logits_list\n",
    "    logits_labels_dict[key] = labels_list\n",
    "\n",
    "# Run martingale\n",
    "results = run_martingale(entropy_streams, protector)\n",
    "\n",
    "# Add accuracy over time\n",
    "for severity_name in label_shift_levels:\n",
    "    key = f\"labelshift_{severity_name}\"\n",
    "    acc_time = compute_accuracy_over_time_from_logits(logits_list_dict[key], logits_labels_dict[key])\n",
    "    results[key][\"accs\"] = acc_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries to store values for each label shift severity\n",
    "log_sj_dict = {}\n",
    "epsilons_dict = {}\n",
    "accuracies_dict = {}\n",
    "entropy_dict = {}\n",
    "\n",
    "# Extract values for each severity level\n",
    "for severity in label_shift_levels.keys():\n",
    "    key = f\"labelshift_{severity}\"\n",
    "    if key in results:\n",
    "        log_sj_dict[severity] = results[key][\"log_sj\"]\n",
    "        epsilons_dict[severity] = results[key][\"eps\"]\n",
    "        accuracies_dict[severity] = results[key][\"accs\"]\n",
    "        entropy_dict[severity] = entropy_streams[key]\n",
    "\n",
    "# Plot the combined results\n",
    "plot_combined_martingale_accuracy_severity(\n",
    "    log_sj_dict,\n",
    "    epsilons_dict,\n",
    "    accuracies_dict,\n",
    "    entropy_dict=entropy_dict,\n",
    "    batch_size=64,\n",
    "    title=\"Label Shift Comparison Across Severities\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(results[\"labelshift_test\"][\"log_sj\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_delays = compute_detection_delays_from_threshold(log_sj_dict)\n",
    "\n",
    "# Call the function to plot detection delays\n",
    "plot_detection_delays(detection_delays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for k in entropy_streams:\n",
    "    sns.kdeplot(entropy_streams[k], label=k)\n",
    "\n",
    "plt.title(\"Entropy Distribution by Label Subset\")\n",
    "plt.xlabel(\"Entropy\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_by_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "fpr_by_size = {}  # ‚Üê new: store FPR per class‚Äëset size\n",
    "\n",
    "for num_classes in [1]:\n",
    "    candidate_class_sets = list(combinations(range(10), num_classes))\n",
    "\n",
    "    entropy_peaks = {}\n",
    "    threshold_crossed = {}\n",
    "\n",
    "    for subset in candidate_class_sets:\n",
    "        print(f\"Evaluating label shift ({num_classes}-class): {subset}\")\n",
    "\n",
    "        # 1) Load synthetic label‚Äëshift stream\n",
    "        x, y = load_cifar10_label_shift(keep_classes=subset, n_examples=8000, shift_point=4000)\n",
    "        dataset = BasicDataset(x, y, transform=transform)\n",
    "        loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "        # 2) Forward pass\n",
    "        ents, acc, logits_list, labels_list = evaluate(model, loader, device)\n",
    "\n",
    "        # 3) Martingale test\n",
    "        key = f\"labelshift_{num_classes}cls_{'_'.join(map(str, subset))}\"\n",
    "        result = run_martingale({key: ents}, protector)[key]\n",
    "\n",
    "        # 4) Record stats\n",
    "        entropy_peaks[key] = np.max(ents)\n",
    "        threshold_crossed[key] = np.max(result[\"log_sj\"]) > np.log(100)\n",
    "\n",
    "    # --- compute per‚Äësize false‚Äëpositive rate -------------------------\n",
    "    n_tests = len(threshold_crossed)\n",
    "    n_crossed = sum(threshold_crossed.values())\n",
    "    fpr = n_crossed / n_tests\n",
    "    fpr_by_size[num_classes] = fpr  # save for later use\n",
    "\n",
    "    print(f\"\\n‚û°Ô∏è  {n_crossed} / {n_tests} ({num_classes}-class) shifts triggered detection  ‚Äì¬†FPR = {fpr:.3f}\\n\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Now fpr_by_size contains something like {2: 0.20, 3: 0.35, ...}\n",
    "# You can save it to disk, log to WandB, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "\n",
    "class PBRSBuffer:\n",
    "    def __init__(self, capacity=64, num_classes=10):\n",
    "        self.capacity = capacity\n",
    "        self.num_classes = num_classes\n",
    "        self.target_per_class = capacity // num_classes\n",
    "        self.buffer = []\n",
    "        self.label_counts = defaultdict(int)\n",
    "\n",
    "    def accept(self, y_hat):\n",
    "        # Always accept if there's room\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            return True\n",
    "        # Reject if class is already full\n",
    "        if self.label_counts[y_hat] >= self.target_per_class:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def add(self, x, entropy, y_hat):\n",
    "        if len(self.buffer) >= self.capacity:\n",
    "            for i, (_, _, label) in enumerate(self.buffer):\n",
    "                if self.label_counts[label] > self.target_per_class:\n",
    "                    self.label_counts[label] -= 1\n",
    "                    del self.buffer[i]\n",
    "                    break\n",
    "            else:\n",
    "                removed = self.buffer.pop(0)\n",
    "                self.label_counts[removed[2]] -= 1\n",
    "\n",
    "        self.buffer.append((x, entropy, y_hat))\n",
    "        self.label_counts[y_hat] += 1\n",
    "\n",
    "    def full(self):\n",
    "        return len(self.buffer) >= self.capacity\n",
    "\n",
    "    def get_entropies(self):\n",
    "        return [entry[1] for entry in self.buffer]\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = []\n",
    "        self.label_counts = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "fpr_by_size = {}  # ‚Üê new: store FPR per class‚Äëset size\n",
    "\n",
    "for num_classes in [3]:\n",
    "    candidate_class_sets = list(combinations(range(10), num_classes))\n",
    "\n",
    "    entropy_peaks = {}\n",
    "    threshold_crossed = {}\n",
    "\n",
    "    for subset in candidate_class_sets:\n",
    "        print(f\"Evaluating label shift ({num_classes}-class): {subset}\")\n",
    "\n",
    "        # 1) Load synthetic label‚Äëshift stream\n",
    "        x, y = load_cifar10_label_shift(keep_classes=subset, n_examples=8000, shift_point=4000)\n",
    "        dataset = BasicDataset(x, y, transform=transform)\n",
    "        loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "        # Step 2: Forward pass with PBRS buffering\n",
    "        confidence_threshold = 0.8  # You can tune this\n",
    "\n",
    "        buffer = PBRSBuffer(capacity=512, num_classes=num_classes)\n",
    "        with torch.no_grad():\n",
    "            for x_batch, _ in loader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                logits = model(x_batch)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                entropies = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)\n",
    "                pseudo_labels = torch.argmax(probs, dim=1)\n",
    "                max_probs = torch.max(probs, dim=1).values  # ‚Üê get model confidence\n",
    "\n",
    "                # Add only confident, accepted samples to buffer\n",
    "                for entropy, y_hat, confidence in zip(\n",
    "                    entropies.cpu().tolist(), pseudo_labels.cpu().tolist(), max_probs.cpu().tolist()\n",
    "                ):\n",
    "                    if confidence > confidence_threshold and buffer.accept(y_hat):\n",
    "                        buffer.add(None, entropy, y_hat)\n",
    "\n",
    "        # Step 3: Run martingale test on buffered entropies\n",
    "        key = f\"labelshift_{num_classes}cls_{'_'.join(map(str, subset))}_PBRS\"\n",
    "\n",
    "        ents = buffer.get_entropies()\n",
    "        result = run_martingale({key: np.array(ents)}, protector)[key]\n",
    "\n",
    "        # Step 4: Store stats\n",
    "        entropy_peaks[key] = np.max(ents)\n",
    "        threshold_crossed[key] = np.max(result[\"log_sj\"]) > np.log(100)\n",
    "\n",
    "    # --- compute per‚Äësize false‚Äëpositive rate -------------------------\n",
    "    n_tests = len(threshold_crossed)\n",
    "    n_crossed = sum(threshold_crossed.values())\n",
    "    fpr = n_crossed / n_tests\n",
    "    fpr_by_size[num_classes] = fpr  # save for later use\n",
    "\n",
    "    print(f\"\\n‚û°Ô∏è  {n_crossed} / {n_tests} ({num_classes}-class) shifts triggered detection  ‚Äì¬†FPR = {fpr:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENTER PARAMETERS ##\n",
    "\n",
    "# Manual settings for arguments\n",
    "args = type(\"Args\", (), {})()  # Create a simple namespace object\n",
    "args.device = \"cpu\"  # Change this manually as needed\n",
    "args.method = \"none\"  # Options: 'none' or 'tent'\n",
    "args.corruption = \"gaussian_noise\"  # Choose from CORRUPTIONS\n",
    "args.all_corruptions = False  # Set to True to test all corruptions\n",
    "args.n_examples = 1000\n",
    "args.batch_size = 64\n",
    "\n",
    "# Set torch seed for replicability (don't know if this preserves consistency when using different devices)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_accept(conf, threshold=0.8, softness=15):\n",
    "    \"\"\"\n",
    "    Accept sample with probability based on confidence.\n",
    "    - High confidence ‚Üí near-certain acceptance\n",
    "    - Medium confidence ‚Üí partial acceptance\n",
    "    - Low confidence ‚Üí mostly rejected\n",
    "    \"\"\"\n",
    "    prob = 1 / (1 + np.exp(-softness * (conf - threshold)))\n",
    "    return random.random() < prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_accept(conf, threshold=0.8, softness=15):\n",
    "    \"\"\"\n",
    "    Accept sample with probability based on confidence.\n",
    "    - High confidence ‚Üí near-certain acceptance\n",
    "    - Medium confidence ‚Üí partial acceptance\n",
    "    - Low confidence ‚Üí mostly rejected\n",
    "    \"\"\"\n",
    "    prob = 1 / (1 + np.exp(-softness * (conf - threshold)))\n",
    "    return random.random() < prob\n",
    "\n",
    "\n",
    "accs, entropy_streams, logits_list_dict, logits_labels_dict = {}, {}, {}, {}\n",
    "results = {}\n",
    "corruptions = CORRUPTIONS if args.all_corruptions else [args.corruption]\n",
    "\n",
    "use_pbrs = True\n",
    "confidence_threshold = 0.8  # You can tune this\n",
    "\n",
    "for corruption in corruptions:\n",
    "    for severity in range(1, 6):\n",
    "        print(f\"üîé {corruption} severity {severity} (clean ‚Üí corrupt)\")\n",
    "\n",
    "        # Load clean-to-corrupt stream\n",
    "        loader, is_clean, labels = load_clean_then_corrupt_sequence(\n",
    "            corruption=corruption,\n",
    "            severity=severity,\n",
    "            n_examples=4000,\n",
    "            data_dir=\"./data\",\n",
    "            transform=transform,\n",
    "            batch_size=args.batch_size,\n",
    "        )\n",
    "\n",
    "        if use_pbrs:\n",
    "            buffer = PBRSBuffer(capacity=512, num_classes=10)\n",
    "\n",
    "        logits_list = []\n",
    "        labels_list = []\n",
    "        entropy_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in loader:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                logits = model(x_batch)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                entropies = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)\n",
    "                pseudo_labels = torch.argmax(probs, dim=1)\n",
    "                max_probs = torch.max(probs, dim=1).values\n",
    "\n",
    "                for entropy, y_hat, conf in zip(\n",
    "                    entropies.cpu().tolist(), pseudo_labels.cpu().tolist(), max_probs.cpu().tolist()\n",
    "                ):\n",
    "                    if use_pbrs:\n",
    "                        if confidence_accept(conf, threshold=0.8, softness=15) and buffer.accept(y_hat):\n",
    "                            buffer.add(None, entropy, y_hat)\n",
    "                    else:\n",
    "                        entropy_list.append(entropy)\n",
    "\n",
    "                logits_list.append(logits.cpu())\n",
    "                labels_list.append(y_batch.cpu())\n",
    "\n",
    "        key = f\"{corruption}_s{severity}\"\n",
    "\n",
    "        # Store entropy stream\n",
    "        ents = np.array(buffer.get_entropies()) if use_pbrs else np.array(entropy_list)\n",
    "        entropy_streams[key] = ents\n",
    "        logits_list_dict[key] = logits_list\n",
    "        logits_labels_dict[key] = labels_list\n",
    "\n",
    "\n",
    "# Run martingale detection on all entropy streams\n",
    "results = run_martingale(entropy_streams, protector)\n",
    "\n",
    "# Compute accuracy over time\n",
    "for corruption in corruptions:\n",
    "    for severity in range(1, 6):\n",
    "        key = f\"{corruption}_s{severity}\"\n",
    "        accs = compute_accuracy_over_time_from_logits(logits_list_dict[key], logits_labels_dict[key])\n",
    "        results[key][\"accs\"] = accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max log_sj values for each severity level\n",
    "max_log_sj = {}\n",
    "for severity in results:\n",
    "    max_log_sj[severity] = max(results[severity][\"log_sj\"])\n",
    "    print(f\"Max log_sj for {severity}: {max_log_sj[severity]}\")\n",
    "\n",
    "# Sort by severity level for cleaner display\n",
    "sorted_max_log_sj = dict(sorted(max_log_sj.items()))\n",
    "\n",
    "# Print results\n",
    "for severity, value in sorted_max_log_sj.items():\n",
    "    print(f\"{severity}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max log_sj values for each severity level\n",
    "max_log_sj = {}\n",
    "for severity in results:\n",
    "    max_log_sj[severity] = max(results[severity][\"log_sj\"])\n",
    "    print(f\"Max log_sj for {severity}: {max_log_sj[severity]}\")\n",
    "\n",
    "# Sort by severity level for cleaner display\n",
    "sorted_max_log_sj = dict(sorted(max_log_sj.items()))\n",
    "\n",
    "# Print results\n",
    "for severity, value in sorted_max_log_sj.items():\n",
    "    print(f\"{severity}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries to store values for each severity\n",
    "log_sj_dict = {}\n",
    "epsilons_dict = {}\n",
    "accuracies_dict = {}\n",
    "\n",
    "# Extract values for each severity level\n",
    "for severity in range(1, 6):\n",
    "    key = f\"gaussian_noise_s{severity}\"\n",
    "    if key in results:\n",
    "        log_sj_dict[key] = results[key][\"log_sj\"]\n",
    "        epsilons_dict[key] = results[key][\"eps\"]\n",
    "        accuracies_dict[key] = results[key][\"accs\"]\n",
    "\n",
    "# Plot the combined results\n",
    "if use_pbrs:\n",
    "    plot_combined_martingale_accuracy_severity_pbrs(\n",
    "        log_sj_dict,\n",
    "        epsilons_dict,\n",
    "        accuracies_dict,\n",
    "        entropy_dict=entropy_streams,\n",
    "        batch_size=64,\n",
    "        buffer_size=256,\n",
    "        title=\"Gaussian Noise Comparison Across Severities (PBRS)\",\n",
    "    )\n",
    "else:\n",
    "    plot_combined_martingale_accuracy_severity(\n",
    "        log_sj_dict,\n",
    "        epsilons_dict,\n",
    "        accuracies_dict,\n",
    "        entropy_dict=entropy_streams,\n",
    "        batch_size=64,\n",
    "        title=\"Gaussian Noise Comparison Across Severities\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Component 2: weighted CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weighted_cdf import *\n",
    "from protector import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically set device to best available option\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Define normalization transform using CIFAR-10 mean and std values\n",
    "transform = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616))\n",
    "\n",
    "# Load pre-trained model and move to appropriate device\n",
    "print(\"üöÄ Loading model...\")\n",
    "model = get_model(args.method, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "\n",
    "# 1. Wrap full dataset\n",
    "# 1) Load synthetic label‚Äëshift stream\n",
    "\n",
    "split = 4000\n",
    "x, y = load_cifar10_label_shift(keep_classes=subset, n_examples=8000, shift_point=split)\n",
    "full_dataset = BasicDataset(x, y, transform=transform)\n",
    "\n",
    "# 2. Split into clean and shifted\n",
    "clean_loader = DataLoader(Subset(full_dataset, range(0, split)), batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    Subset(full_dataset, range(split, len(full_dataset))), batch_size=args.batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "# 1. Estimate p_s from clean CIFAR-10\n",
    "p_s = estimate_label_distribution(model, clean_loader, device)\n",
    "\n",
    "# 2. Estimate p_t and collect pseudo-labels on test stream\n",
    "p_t, _ = estimate_pseudo_label_distribution(model, test_loader, device)\n",
    "_, source_pseudo_labels = estimate_pseudo_label_distribution(model, clean_loader, device)\n",
    "\n",
    "# 3. Collect entropies on clean source set\n",
    "# source_ents = compute_entropies(model, clean_loader, device)  # or reuse from before\n",
    "source_ents = []\n",
    "source_pseudo_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, _ in clean_loader:\n",
    "        x = x.to(device)\n",
    "        logits = model(x)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)\n",
    "        y_hat = torch.argmax(probs, dim=1)\n",
    "\n",
    "        source_ents.extend(entropy.cpu().tolist())\n",
    "        source_pseudo_labels.extend(y_hat.cpu().tolist())\n",
    "\n",
    "# 4. Create weighted CDF object\n",
    "weighted_cdf = WeightedCDF(entropies=source_ents, pseudo_labels=source_pseudo_labels, p_s=p_s.numpy(), p_t=p_t.numpy())\n",
    "protector = protect.get_weighted_protector_from_ents(\n",
    "    source_ents,\n",
    "    source_pseudo_labels,\n",
    "    p_s,\n",
    "    p_t,\n",
    "    argparse.Namespace(gamma=1 / (8 * np.sqrt(3)), eps_clip=1.8, device=device),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted CDF with BBSE weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from weighted_cdf_bbse import BBSEWeightedCDF, estimate_shift_weights\n",
    "\n",
    "# 1. Wrap full dataset\n",
    "# 1) Load synthetic label‚Äëshift stream\n",
    "\n",
    "split = 4000\n",
    "x, y = load_cifar10_label_shift(keep_classes=subset, n_examples=8000, shift_point=split)\n",
    "full_dataset = BasicDataset(x, y, transform=transform)\n",
    "\n",
    "# 2. Split into clean and shifted\n",
    "clean_loader = DataLoader(Subset(full_dataset, range(0, split)), batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    Subset(full_dataset, range(split, len(full_dataset))), batch_size=args.batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "# nice wrapper function takes care of evertyhing\n",
    "w, p_s, p_t_true, p_t_pred = estimate_shift_weights(model, clean_loader, test_loader, device)\n",
    "\n",
    "# 3. Collect entropies on clean source set\n",
    "# Could probably integrate entropy calculation into estimate_shift_weights for efficiency\n",
    "source_ents = compute_entropies(model, clean_loader, device)  # or reuse from before\n",
    "\n",
    "# source_ents = []\n",
    "# source_pseudo_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for x, _ in clean_loader:\n",
    "#         x = x.to(device)\n",
    "#         logits = model(x)\n",
    "#         probs = torch.softmax(logits, dim=1)\n",
    "#         entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)\n",
    "#         #y_hat = torch.argmax(probs, dim=1)\n",
    "\n",
    "#         source_ents.extend(entropy.cpu().tolist())\n",
    "#         #source_pseudo_labels.extend(y_hat.cpu().tolist())\n",
    "\n",
    "# 4. Create weighted CDF object\n",
    "weighted_cdf = BBSEWeightedCDF(\n",
    "    entropies=source_ents,\n",
    "    pseudo_labels=pseudo_labels,\n",
    "    weights=weights,\n",
    ")\n",
    "\n",
    "protector = protect.get_weighted_protector_from_ents(\n",
    "    source_ents,\n",
    "    source_pseudo_labels,\n",
    "    p_s,\n",
    "    p_t,\n",
    "    argparse.Namespace(gamma=1 / (8 * np.sqrt(3)), eps_clip=1.8, device=device),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from weighted_cdf import *\n",
    "\n",
    "fpr_by_size = {}\n",
    "\n",
    "for num_classes in [2]:  # Adjust as needed\n",
    "    candidate_class_sets = list(combinations(range(10), num_classes))\n",
    "    entropy_peaks = {}\n",
    "    threshold_crossed = {}\n",
    "\n",
    "    for subset in candidate_class_sets[:3]:\n",
    "        print(f\"\\nüîé Evaluating label shift ({num_classes}-class): {subset}\")\n",
    "\n",
    "        split = 4000\n",
    "\n",
    "        # 1) Load clean source stream using only this subset of classes\n",
    "        x_src, y_src = load_cifar10_label_shift(keep_classes=subset, n_examples=8000, shift_point=split)\n",
    "        source_dataset = BasicDataset(x_src, y_src, transform=transform)\n",
    "\n",
    "        source_loader = DataLoader(Subset(source_dataset, range(0, split)), batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "        # 2) Estimate p_s and collect source entropies + pseudo-labels\n",
    "        p_s = estimate_label_distribution(model, source_loader, device)\n",
    "\n",
    "        source_ents = []\n",
    "        source_pseudo_labels = []\n",
    "        with torch.no_grad():\n",
    "            for x_batch, _ in source_loader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                logits = model(x_batch)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)\n",
    "                y_hat = torch.argmax(probs, dim=1)\n",
    "\n",
    "                source_ents.extend(entropy.cpu().tolist())\n",
    "                source_pseudo_labels.extend(y_hat.cpu().tolist())\n",
    "\n",
    "        # 3) Load label-shifted stream (same subset)\n",
    "        x_shift, y_shift = load_cifar10_label_shift(keep_classes=subset, n_examples=8000, shift_point=split)\n",
    "        full_dataset = BasicDataset(x_shift, y_shift, transform=transform)\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            Subset(full_dataset, range(split, len(full_dataset))), batch_size=args.batch_size, shuffle=False\n",
    "        )\n",
    "\n",
    "        # 4) Estimate p_t from shifted half of the stream\n",
    "        p_t, _ = estimate_pseudo_label_distribution(model, test_loader, device)\n",
    "\n",
    "        mask = torch.tensor([i in subset for i in range(10)], dtype=torch.bool)\n",
    "\n",
    "        p_s = p_s * mask\n",
    "        p_s /= p_s.sum() + 1e-8\n",
    "\n",
    "        p_t = p_t * mask\n",
    "        p_t /= p_t.sum() + 1e-8\n",
    "\n",
    "        # 5) Create weighted protector\n",
    "        protector = protect.get_weighted_protector_from_ents(\n",
    "            source_ents,\n",
    "            source_pseudo_labels,\n",
    "            p_s,\n",
    "            p_t,\n",
    "            argparse.Namespace(gamma=1 / (8 * np.sqrt(3)), eps_clip=1.8, device=device),\n",
    "        )\n",
    "\n",
    "        # 6) Evaluate full test stream and run martingale\n",
    "        loader = DataLoader(full_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "        ents, acc, logits_list, labels_list = evaluate(model, loader, device)\n",
    "\n",
    "        key = f\"labelshift_{num_classes}cls_{'_'.join(map(str, subset))}\"\n",
    "        result = run_martingale({key: ents}, protector)[key]\n",
    "\n",
    "        # 7) Record statistics\n",
    "        entropy_peaks[key] = np.max(ents)\n",
    "        threshold_crossed[key] = np.max(result[\"log_sj\"]) > np.log(100)\n",
    "\n",
    "        print(\"Subset:\", subset)\n",
    "        print(\"p_s:\", p_s.numpy())\n",
    "        print(\"p_t:\", p_t.numpy())\n",
    "        print(\"min p_s:\", p_s.min().item(), \"min p_t:\", p_t.min().item())\n",
    "\n",
    "    # 8) Calculate and store FPR\n",
    "    n_tests = len(threshold_crossed)\n",
    "    n_crossed = sum(threshold_crossed.values())\n",
    "    fpr = n_crossed / n_tests\n",
    "    fpr_by_size[num_classes] = fpr\n",
    "\n",
    "    print(f\"\\n‚û°Ô∏è {n_crossed} / {n_tests} ({num_classes}-class) shifts triggered detection ‚Äî FPR = {fpr:.3f}\\n\")\n",
    "\n",
    "# fpr_by_size now holds clean, valid FPRs per class subset size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BBSE False positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from weighted_cdf_bbse import BBSEWeightedCDF, estimate_shift_weights\n",
    "\n",
    "fpr_by_size = {}\n",
    "\n",
    "for num_classes in [2]:  # Adjust as needed\n",
    "    candidate_class_sets = list(combinations(range(10), num_classes))\n",
    "    entropy_peaks = {}\n",
    "    threshold_crossed = {}\n",
    "\n",
    "    for subset in candidate_class_sets[:3]:\n",
    "        print(f\"\\nüîé Evaluating label shift ({num_classes}-class): {subset}\")\n",
    "\n",
    "        split = 4000\n",
    "\n",
    "        # 1) Load clean source stream using only this subset of classes\n",
    "        x_src, y_src = load_cifar10_label_shift(keep_classes=subset, n_examples=8000, shift_point=split)\n",
    "        source_dataset = BasicDataset(x_src, y_src, transform=transform)\n",
    "\n",
    "        source_loader = DataLoader(Subset(source_dataset, range(0, split)), batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "        source_ents = []\n",
    "        source_labels = []\n",
    "        with torch.no_grad():\n",
    "            for x_batch, labels in source_loader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                logits = model(x_batch)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)\n",
    "                y_hat = torch.argmax(probs, dim=1)\n",
    "\n",
    "                source_ents.extend(entropy.cpu().tolist())\n",
    "                # We have access to ground truth labels for init of CDF, so we are using them here\n",
    "                source_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "        # 3) Load label-shifted stream (same subset)\n",
    "        x_shift, y_shift = load_cifar10_label_shift(keep_classes=subset, n_examples=8000, shift_point=split)\n",
    "        full_dataset = BasicDataset(x_shift, y_shift, transform=transform)\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            Subset(full_dataset, range(split, len(full_dataset))), batch_size=args.batch_size, shuffle=False\n",
    "        )\n",
    "\n",
    "        # 4) Estimate p_t from shifted half of the stream\n",
    "        weights, p_s, p_t_true, p_t_pred = estimate_shift_weights(model, source_loader, test_loader, device)\n",
    "        print(f\"p_true: {p_t_true}\")\n",
    "\n",
    "        # 5) Create weighted protector\n",
    "        protector = protect.get_bbse_weighted_protector_from_ents(\n",
    "            source_ents,\n",
    "            source_labels,\n",
    "            weights,\n",
    "            argparse.Namespace(gamma=1 / (8 * np.sqrt(3)), eps_clip=1.8, device=device),\n",
    "        )\n",
    "\n",
    "        # 6) Evaluate full test stream and run martingale\n",
    "        loader = DataLoader(full_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "        ents, acc, logits_list, labels_list = evaluate(model, loader, device)\n",
    "\n",
    "        key = f\"labelshift_{num_classes}cls_{'_'.join(map(str, subset))}\"\n",
    "        result = run_martingale({key: ents}, protector)[key]\n",
    "\n",
    "        # 7) Record statistics\n",
    "        entropy_peaks[key] = np.max(ents)\n",
    "        threshold_crossed[key] = np.max(result[\"log_sj\"]) > np.log(100)\n",
    "\n",
    "        print(\"Subset:\", subset)\n",
    "        print(\"p_s:\", p_s.numpy())\n",
    "        # TODO: LOUIS: Not sure if we should be using the original preds or the transformed ones\n",
    "        print(\"p_t:\", p_t_true.numpy())\n",
    "        print(\"min p_s:\", p_s.min().item(), \"min p_t:\", p_t_true.min().item())\n",
    "\n",
    "    # 8) Calculate and store FPR\n",
    "    n_tests = len(threshold_crossed)\n",
    "    n_crossed = sum(threshold_crossed.values())\n",
    "    fpr = n_crossed / n_tests\n",
    "    fpr_by_size[num_classes] = fpr\n",
    "\n",
    "    print(f\"\\n‚û°Ô∏è {n_crossed} / {n_tests} ({num_classes}-class) shifts triggered detection ‚Äî FPR = {fpr:.3f}\\n\")\n",
    "\n",
    "# fpr_by_size now holds clean, valid FPRs per class subset size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BBSE ODS False Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from weighted_cdf_bbse_ods import (\n",
    "    BBSEODSWeightedCDF,\n",
    "    estimate_confusion_matrix,\n",
    "    estimate_target_distribution_from_preds,\n",
    ")\n",
    "\n",
    "fpr_by_size = {}\n",
    "\n",
    "for num_classes in [2]:  # Adjust as needed\n",
    "    candidate_class_sets = list(combinations(range(10), num_classes))\n",
    "    entropy_peaks = {}\n",
    "    threshold_crossed = {}\n",
    "\n",
    "    for subset in candidate_class_sets[:3]:\n",
    "        print(f\"\\nüîé Evaluating label shift ({num_classes}-class): {subset}\")\n",
    "\n",
    "        split = 4000\n",
    "\n",
    "        # 1) Load clean source stream using only this subset of classes\n",
    "        x_src, y_src = load_cifar10_label_shift(keep_classes=subset, n_examples=8000, shift_point=split)\n",
    "        source_dataset = BasicDataset(x_src, y_src, transform=transform)\n",
    "\n",
    "        source_loader = DataLoader(Subset(source_dataset, range(0, split)), batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "        source_ents = []\n",
    "        source_labels = []\n",
    "        with torch.no_grad():\n",
    "            for x_batch, labels in source_loader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                logits = model(x_batch)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)\n",
    "                y_hat = torch.argmax(probs, dim=1)\n",
    "\n",
    "                source_ents.extend(entropy.cpu().tolist())\n",
    "                # We have access to ground truth labels for init of CDF, so we are using them here\n",
    "                source_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "        # BBSE: Estimate confusion matrix and p_source from clean source stream\n",
    "        confusion_matrix, p_source = estimate_confusion_matrix(model, source_loader, device)\n",
    "\n",
    "        # Load label-shifted stream (same subset)\n",
    "        x_shift, y_shift = load_cifar10_label_shift(keep_classes=subset, n_examples=8000, shift_point=split)\n",
    "        full_dataset = BasicDataset(x_shift, y_shift, transform=transform)\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            Subset(full_dataset, range(split, len(full_dataset))), batch_size=args.batch_size, shuffle=False\n",
    "        )\n",
    "\n",
    "        # 4. Estimate p_test from shifted half of the stream\n",
    "        p_test = estimate_target_distribution_from_preds(model, test_loader, device)\n",
    "\n",
    "        # 5. Create BBSE/ODS weighted protector\n",
    "        protector = protect.get_bbse_ods_weighted_protector_from_ents(\n",
    "            source_ents,\n",
    "            p_test,\n",
    "            p_source,\n",
    "            source_labels,\n",
    "            confusion_matrix,\n",
    "            0.05,  # ods_alpha\n",
    "            argparse.Namespace(gamma=1 / (8 * np.sqrt(3)), eps_clip=1.8, device=device),\n",
    "        )\n",
    "\n",
    "        # LOUIS: This is for stat tracking i guess?\n",
    "        mask = torch.tensor([i in subset for i in range(10)], dtype=torch.bool)\n",
    "        p_source = p_source.to(\"cpu\") * mask\n",
    "        p_source /= p_source.sum() + 1e-8\n",
    "\n",
    "        p_test_true = protector.cdf.p_test_true.to(\"cpu\") * mask\n",
    "        p_test_true /= p_test_true.sum() + 1e-8\n",
    "\n",
    "        # LOUIS: Here's where the problem is... i think the evaluate model logic and run martingale logic need to be\n",
    "        # combined somehow, in the current setup the protector CDF cannot be updated as we go along, i think? \n",
    "        # 6) Evaluate full test stream and run martingale\n",
    "        loader = DataLoader(full_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "        ents, acc, logits_list, labels_list = evaluate(model, loader, device)\n",
    "\n",
    "        key = f\"labelshift_{num_classes}cls_{'_'.join(map(str, subset))}\"\n",
    "        result = run_martingale({key: ents}, protector)[key]\n",
    "\n",
    "        # Record statistics\n",
    "        entropy_peaks[key] = np.max(ents)\n",
    "        threshold_crossed[key] = np.max(result[\"log_sj\"]) > np.log(100)\n",
    "\n",
    "        print(\"Subset:\", subset)\n",
    "        print(\"p_source:\", p_source.numpy())\n",
    "        print(\"p_test_true:\", p_test_true.numpy())\n",
    "        print(\"min p_source:\", p_source.min().item(), \"min p_t:\", p_test_true.min().item())\n",
    "\n",
    "    # 8) Calculate and store FPR\n",
    "    n_tests = len(threshold_crossed)\n",
    "    n_crossed = sum(threshold_crossed.values())\n",
    "    fpr = n_crossed / n_tests\n",
    "    fpr_by_size[num_classes] = fpr\n",
    "\n",
    "    print(f\"\\n‚û°Ô∏è {n_crossed} / {n_tests} ({num_classes}-class) shifts triggered detection ‚Äî FPR = {fpr:.3f}\\n\")\n",
    "\n",
    "# fpr_by_size now holds clean, valid FPRs per class subset size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs, entropy_streams, logits_list_dict, logits_labels_dict = {}, {}, {}, {}\n",
    "results = {}\n",
    "corruptions = CORRUPTIONS if args.all_corruptions else [args.corruption]\n",
    "\n",
    "use_pbrs = False\n",
    "confidence_threshold = 0.8  # You can tune this\n",
    "\n",
    "for corruption in corruptions:\n",
    "    for severity in range(1, 6):\n",
    "        print(f\"üîé {corruption} severity {severity} (clean ‚Üí corrupt)\")\n",
    "\n",
    "        # Load clean-to-corrupt stream\n",
    "        loader, is_clean, labels = load_clean_then_corrupt_sequence(\n",
    "            corruption=corruption,\n",
    "            severity=severity,\n",
    "            n_examples=4000,\n",
    "            data_dir=\"./data\",\n",
    "            transform=transform,\n",
    "            batch_size=args.batch_size,\n",
    "        )\n",
    "\n",
    "        if use_pbrs:\n",
    "            buffer = PBRSBuffer(capacity=512, num_classes=10)\n",
    "\n",
    "        logits_list = []\n",
    "        labels_list = []\n",
    "        entropy_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in loader:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                logits = model(x_batch)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                entropies = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)\n",
    "                pseudo_labels = torch.argmax(probs, dim=1)\n",
    "                max_probs = torch.max(probs, dim=1).values\n",
    "\n",
    "                for entropy, y_hat, conf in zip(\n",
    "                    entropies.cpu().tolist(), pseudo_labels.cpu().tolist(), max_probs.cpu().tolist()\n",
    "                ):\n",
    "                    if use_pbrs:\n",
    "                        if confidence_accept(conf, threshold=0.8, softness=15) and buffer.accept(y_hat):\n",
    "                            buffer.add(None, entropy, y_hat)\n",
    "                    else:\n",
    "                        entropy_list.append(entropy)\n",
    "\n",
    "                logits_list.append(logits.cpu())\n",
    "                labels_list.append(y_batch.cpu())\n",
    "\n",
    "        key = f\"{corruption}_s{severity}\"\n",
    "\n",
    "        # Store entropy stream\n",
    "        ents = np.array(buffer.get_entropies()) if use_pbrs else np.array(entropy_list)\n",
    "        entropy_streams[key] = ents\n",
    "        logits_list_dict[key] = logits_list\n",
    "        logits_labels_dict[key] = labels_list\n",
    "\n",
    "\n",
    "# Run martingale detection on all entropy streams\n",
    "results = run_martingale(entropy_streams, protector)\n",
    "\n",
    "# Compute accuracy over time\n",
    "for corruption in corruptions:\n",
    "    for severity in range(1, 6):\n",
    "        key = f\"{corruption}_s{severity}\"\n",
    "        accs = compute_accuracy_over_time_from_logits(logits_list_dict[key], logits_labels_dict[key])\n",
    "        results[key][\"accs\"] = accs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cifar100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
